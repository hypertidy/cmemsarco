---
output: github_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

# cmemsarco
 
Cloud-native access to Copernicus Marine (CMEMS) ARCO Zarr stores. No file downloads, no directory listings, no NetCDF wrangling - just URLs and GDAL.
 
## Installation

```{r}
# install.packages("pak")
pak::pak("mdsumner/cmemsarco")
```

## The idea

CMEMS provides Analysis-Ready Cloud-Optimized (ARCO) Zarr stores for their marine datasets. These are chunked for two access patterns:

| Bucket | Zarr | Chunks | Use case |
|--------|------|--------|----------|
| `mdl-arco-geo-*` | `geoChunked.zarr` | (138, 32, 64) | Time series at a point |
| `mdl-arco-time-*` | `timeChunked.zarr` | (1, 720, 512) | Spatial slice at one time |

The S3 buckets don't allow LIST operations, but GDAL's Zarr driver doesn't need them - it reads `/.zmetadata` and derives chunk paths from the Zarr spec. This means you can go straight from URL to pixels with no intermediate steps.

## Usage

cmemsarco comes with a ready to use catalog: 

```{r catalog}
library(cmemsarco)
data(cmems_catalog_data)
```

See [Build a catalog](#build-a-catalog) for building this from scratch. 


### Get a GDAL source

```{r}
# Filter to what you need
sla <- catalog |>
  dplyr::filter(product_id == "SEALEVEL_GLO_PHY_L4_NRT_008_046") |>
  cmems_latest()  # latest version per dataset

# Get the GDAL-ready DSN
dsn <- sla$timeChunked_gdal[1]
dsn
#> 'ZARR:"/vsis3/mdl-arco-time-045/arco/SEALEVEL_GLO_PHY_L4_NRT_008_046/cmems_obs-sl_glo_phy-ssh_nrt_allsat-l4-duacs-0.25deg_P1D_202411/timeChunked.zarr"'
```

### Read data directly

Set up the environment once per session:

```{r}
cmems_setup()  
# Sets AWS_NO_SIGN_REQUEST=YES, AWS_S3_ENDPOINT=s3.waw3-1.cloudferro.com
```

Then use your preferred GDAL interface:

```{r}
library(vapour)

# List available arrays/variables
vapour_sds_names(dsn)
#> [1] 'ZARR:".../timeChunked.zarr":/sla'
#> [2] 'ZARR:".../timeChunked.zarr":/adt'
#> [3] 'ZARR:".../timeChunked.zarr":/ugos'
#> ...

# Get var-specific DSN
sla_dsn <- cmems_gdal_dsn(sla$timeChunked_url[1], array = "sla")

# Read var info

str(jsonlite::fromJSON(gdalraster::mdim_info(dsn, cout = FALSE))[1:4])


# Read a spatial subset at specific time index (band)
extent <- c(100, 160, -50, 0)  
band <- 1000  # time index
dat <- vapour_warp_raster(
 sla_dsn, 
 extent = extent,
 dimension = c(512, 512),
 bands = band
)
```

With terra:

```{r}
library(terra)

r <- rast(sla_dsn)
r
#> class       : SpatRaster 
#> dimensions  : 720, 1440, 10000+ (nrow, ncol, nlyr)
#> ...

# Subset by extent and time
r_sub <- r[[1000:1010]] |> 
 crop(ext(100, 160, -50, 0))
```

### Direct URL construction

If you already know the product, dataset, and version (e.g. from `copernicusmarine describe`), skip the catalog:
 
```{r}
dsn <- cmems_arco_dsn(
 product_id = "SEALEVEL_GLO_PHY_L4_NRT_008_046",
 dataset_id = "cmems_obs-sl_glo_phy-ssh_nrt_allsat-l4-duacs-0.25deg_P1D",
 version = "202411",
 chunk_type = "time",
 array = "sla"
)
```

Note: This requires knowing the bucket version suffix (e.g., "045") which varies by product family. The catalog approach handles this automatically.

## Why this works

CMEMS ARCO infrastructure:

```
https://s3.waw3-1.cloudferro.com/
 └── mdl-arco-{time|geo}-{NNN}/
       └── arco/
             └── {PRODUCT_ID}/
                   └── {dataset_id}_{version}/
                         └── {time|geo}Chunked.zarr/
                               ├── .zmetadata
                               ├── .zattrs  
                               └── {variable}/{chunk_indices}
```

GDAL with `/vsis3/` reads `.zmetadata` to understand the array structure, then fetches only the chunks needed for your read operation. No LIST calls, no full downloads.

The STAC catalog at `https://stac.marine.copernicus.eu/metadata/catalog.stac.json` provides the authoritative mapping from product/dataset to actual S3 URLs.

## Chunk strategy

Choose your Zarr based on access pattern:

**timeChunked** (chunks: 1 × 720 × 512 in time × lat × lon)
- Spatial slices: maps at one or few time steps
- Efficient for: `crop()`, regional extracts, spatial analysis

**geoChunked** (chunks: 138 × 32 × 64 in time × lat × lon)
- Time series: values at one or few locations over many times
- Efficient for: point extraction, time series analysis

Wrong chunk type = many more HTTP requests = slow.


### Build a catalog

Walk the STAC catalog to get all products, datasets, and their Zarr URLs:

```{r build-catalog, eval=TRUE}
library(cmemsarco)

# Get everything (takes several minutes)
# catalog <- cmems_catalog()

# Or specific products
catalog <- cmems_catalog(
  product_ids = c(
    "SEALEVEL_GLO_PHY_L4_NRT_008_046",
    "GLOBAL_ANALYSISFORECAST_PHY_001_024"
  )
)

# Filter to ARCO-only (drops static/native-only datasets)
#catalog <- cmems_arco_only(catalog)

catalog

```



## Related

- [CopernicusMarine](https://github.com/pepijn-devries/CopernicusMarine) - R interface for CMEMS (WMTS, native files, subsetting via their API)
- [copernicusmarine](https://pypi.org/project/copernicusmarine/) - Official Python toolbox
- [vapour](https://github.com/hypertidy/vapour) - Lightweight GDAL
- [ZarrDatasets.jl](https://github.com/JuliaGeo/ZarrDatasets.jl) - Julia equivalent (where I found clear STAC examples)

## Data source

EU Copernicus Marine Service Information. See individual products for citation requirements.
